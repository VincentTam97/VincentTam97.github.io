---
layout:     post
title:      一周Papers
subtitle:   2019/3/6 至 2019/3/8
date:       2019-03-10
author:     Vincent Tam
header-img: img/post-190310-1.jpg
catalog: true
tags:
    - Paper
    - NLP
    - Deep Learning
---

## 前言

近期语言模型成为 NLP 的热门讨论话题。从前两年提出的Transformer，到去年的ELMo、BERT，都引起了各个方向研究者的极大兴趣。因为他们作为各种task的上游预训练模型，可以达到惊人的效果，后续的下游任务只需要工作者进行一定程度的 fine-tuning 即可。因此，这种 pre-training & fine-tuning 的模式在最近也是一种十分常见的训练方式。这周看的两篇论文都与这个相关，包括语言模型 BERT 在 summary generation 中的利用，以及今年年初研究者所提出的一种全新的非监督方式训练多任务语言模型的做法。本周想多读几篇的，结果周末又划水了，小亏。

-----


## Pretraining-Based Natural Language Generation for Text Summarization

### 基本思想

本文提出了一个基于与训练的 encoder-decoder 框架。模型的 encoder 部分，作者将输入序列通过BERT模型得到文本的一个表示。在模型的 decoder 部分，作者分为了两个步骤：第一步使用一个基于 Transformer 的 decoder 来生成一个临时的（draft）输出序列，第二步，将临时输出序列中的每一个词遮住，并将其喂给 BERT，然后让其来预测这个词。模型的目标是最大化能生成正确目标序列的概率。

### 主要贡献

1. 提出了一个基于 BERT 的语言生成模型，在 encoder 和 decoder 中善加利用预训练的语言模型，整个过程中不需要人工的特征标记。
2. 设计了一个两阶段的 decoder 过程。
3. 在数据集 benchmark 上跑分，达到了一定的成绩。

### 模型介绍

![](https://raw.githubusercontent.com/VincentTam97/_BlogImgStorage/master/images/20190308112429.png)

上图是模型的 overview，可以看到图的左边部分，encoder 部分使用 BERT，得到文档的嵌入表示。中间部分是 decoder 的阶段1，代表一个 summary draft decoder，用于生成临时的输出序列。右边部分又是一个 BERT，利用其得到前一步 mask 后的 summary draft output 的嵌入表示，联合之前的文档嵌入表示，一同喂给 multi-head attention decoder ，最终得到需要的 summary output。

-----


## Language Models are Unsupervised Multitask Learners

### 基本思想

由于自然语言处理中的任务（如机器翻译、问答系统、文档摘要等）都需要特定的语料库以监督学习的方式来做训练，从而能够得到一个在特定任务下的预测器。但是作者通过实验证明，如文章标题所述，语言模型可以是一个“多才多能”的选手，即同一个语言模型可以应对多个不同的任务。并且，作者在训练这一语言模型的过程中，使用的是非监督学习的训练方法，即训练集没有做人工的标注。

### 主要贡献

1. 提出了他们所训练的多任务语言模型 GPT-2。
2. 训练的语料由于需要处理多个不同的自然语言任务，因此训练内容以多元组的方式给出。元组中给出了所需要达成的任务，以及这一验证这一任务所需要的多项语料。

### 实验部分

作者利用他们的 GPT-2 模型在以下几个任务中进行了实验：language modeling、children's book test、LAMBADA、reading conprehension、summarization、translation、question answering 等。实验的结果在原论文中都有给出。GPT-2 在多个NLP的任务中都取得了相当不错的效果，达到了多个SOTA，证明了作者的说法，即可以通过非监督的方式训练一个单一的语言模型来完成多个任务。