---
layout:     post
title:      一周Papers（2）
subtitle:   2019/3/11 至 2019/3/14
date:       2019-03-14
author:     Vincent Tam
header-img: img/post-190310-1.jpg
catalog: true
tags:
    - Paper
    - NLP
    - Deep Learning
    - Dialogue Generation
---

## 前言

本周看了三篇对话生成相关的论文，都注重不同的点。可借鉴性的话，前两篇高一些，最后一篇主要是针对于面向目标的对话生成。不过最后一篇结合了监督学习和强化学习来进行对话生成，mask 的思想还是挺有意思的。这周提前在周四发出来一周的 Papers，因为后面几天有点事需要忙。不过，如果有新的东西我会尽快分享啦。



-----



## Exemplar Encoder-Decoder for Neural Conversation Generation

### 基本思想

对话生成模型分为两类。一类是基于生成的模型（generative models），另一类是基于检索的模型（retrieval models）。基于生成的模型将问题看作一个对话的生成，进而转化为 sequence to sequence 的学习问题。基于检索的模型将对话的上文/背景内容看作是一个检索请求（query），并想办法在对话记录中检索到答案。

本文的思路与生成模型和检索模型都相关，但都有不同方向的改进。与大部分的生成模型不同的地方在于，本文使用了“相似的对话上下文”用于训练 response。

![](https://raw.githubusercontent.com/VincentTam97/_BlogImgStorage/master/images/20190311161429.png)

上表中，第一列是输入的上（下）文，或者说是当前的对话语境，模型需要基于这个语境给出一个合理的回答。第二列可以看作是对于当前语境的一个回答的 ground truth。第三和第四列则是作者创新引入的“相似的对话上下文”以及对应的回答。

> 因此，我们不是让seq2seq模型仅从对齐的平行语料库中学习语言模式，而是通过在生成文本时从训练数据中提供密切相关（相似）的样本来辅助模型。

### 主要贡献

1. 提出了一个创新的 Exemplar Encoder-Decoder(EED) 架构，使得我们可以使用“相似的上下文对话”来训练模型。
2. 进行了详尽又完备的实验（作者自己说的），在公开的 Ubuntu dialog corpus 数据集的benchmark 进行了实验。在多个评价指标上均超过了作者所引用的几个他人的成果（但没说是不是SOTA）。

### 模型架构

作者利用信息检索的方法获取了一些 context-response 对，用于训练。输入文本（语境）和这些 context-response 对，被喂给 EED 网络。这个网络的结构如下。

![](https://raw.githubusercontent.com/VincentTam97/_BlogImgStorage/master/images/20190311164429.png)

EED encoder 将输入上下文和检索到的 context-response 对组合在一起以创建一组示例向量。然后，EED decoder 基于输入上下文和检索到的上下文之间的相似性来使用 exemplar 的向量，以生成response。

![](https://raw.githubusercontent.com/VincentTam97/_BlogImgStorage/master/images/20190311165345.JPG)



-----



## NEXUS Network: Connecting the Preceding and the Following in Dialogue Generation

### 基本思想

如何解决对话生成过程中，为了保证对话生成和合理和通顺，而导致的可能生成无意义回答（例如“我不知道”等）的问题？作者表示，需要建立当前节点与上文和下文的连接。建立这个连接需要通过最大化相互信息（maximizing mutual infomation, MMI）来实现。通过这种方式，无意义回答（或者称为“通用”回答）将不被鼓励，因为不含任何有价值的信息，并且与上文的关系连接不紧密。为了实现有意义的对话系统训练，作者提出了两个挑战。

第一个挑战是自然语言 token 的离散性，不便于我们进行有效的梯度下降。第二个挑战是，对于某个时间节点来说，后文的内容是未知的。作者提出了其“辅助连续代码空间”的概念（auxiliary continuous code space）。

在每个时间步，不是直接优化离散的对话，而是训练当前，过去和将来的对话以最大化与该代码空间的互信息。此外，可以同时优化可学习的先验分布以预测相应的代码空间，从而能够在测试阶段进行有效的采样，而无需访问真实的未来对话（即下文）。

### 主要贡献

1. 提出了一个 continuous code space 的概念，作者表示 code space 的训练是用于推断 encoded history H~i-1~ 和 future F~i+1~ 。

### 模型介绍

![](https://raw.githubusercontent.com/VincentTam97/_BlogImgStorage/master/images/20190312214419.png)

> 实线表示生成连续代码和相应响应的生成模型。虚线表示推理模型，其中训练后验代码以推断历史，当前和未来的对话。两个部分同时通过梯度下降进行训练。



为了最大化互信息，作者引入了以下表达。

![](https://raw.githubusercontent.com/VincentTam97/_BlogImgStorage/master/images/20190312214943.JPG)

因此作者将 U~i~ 替换为连续的 code space C，它是由整个对话流学习出来的。过程如上上图中的流程所示。因此 encoding 阶段实际上我们需要的是这个 c，也是被喂给 decoder 的内容。decoder 拿到这个 c 之后，将其转换为我们需要的对话内容。

![](https://raw.githubusercontent.com/VincentTam97/_BlogImgStorage/master/images/20190312215638.JPG)

具体的目标函数公式推导过程比较复杂，我没仔细读了。总的来说，作者干了这么一件事：将不便于BP训练的离散token，先通过 encoder 转化成某种 code space c，使其可以被学，然后将其喂给 decoder。

### 实验部分

作者与以往的模型进行了对比实验，的确领先于别的模型，但我感觉进步幅度很小，没有很显著的效果，可能与各种超参的调整都有关系。



-----



## Learning End-to-End Goal-Oriented Dialog with Multiple Answers

### 基本思想

在一个对话 flow 中，下一句对话的内容，可能有多个合理的选择。目前的端到端神经网络方法并没有考虑这种情况，那些模型均基于任何时候下一句的回答只有一个正确选择的假设。作者表示，他们可以有多种方法在基于目标的对话系统中实现这一目标，推出了一种结合监督学习和强化学习的新方法。为了检验他们的方法效果，他们还特意推出了一个新的测试任务，permuted-bAbI dialog tasks，以往的模型在这一任务上的测试效果远差于他们的新模型。

训练对话机器人的方式基本可以分为 SL （监督学习）和 RL （强化学习）两种。强化学习中，我们难以定义和奖励什么是合适的回答，因而从零开始学习如何对话是一件不太可能的事情。由于 RL 需要大量的训练交互，事实上需要 SL 的方法加以补充。而 SL 的方法，训练集也不会说在任何一个对话节点，下一句对话都提供多种可能性，训练梯度的时候基于下一句对话只有一种正确答案的假设，因此也不能实现我们的要求。

### 主要贡献

1. 作者提出了他们用于解决在面向目标（goal-oriented）的对话中，考虑 multiple-utterance 的问题的方法。
2. 提出了一个新的测试方法用于这一任务。

### 方法介绍

作者所提出的方法分为两个阶段。第一个阶段，对话系统通过给定的数据集尝试学习如何模仿对话，第二阶段的学习通过试错和（人类给予的）错误警告来学习。第一阶段使用的是监督学习（SL），而第二阶段是强化学习（RL）。

在 SL 阶段期间，在每个数据点处，对话系统被训练以产生在该数据点中提供的下一个对话，并且即使它产生其他有效的下一个对话之一也会受到惩罚。我们通过为对话系统提供：仅使用状态向量的一部分来产生特定的下一个话语的能力，来避免这种情况。我的理解是，这样的话，状态向量的另一个部分便不会受到影响，从而可以进行试错。作者通过生成一个 mask 向量 m，用于决定状态向量 s 的哪一个部分要被遮住，即哪一部分用于产生对话的回答。

在 RL 阶段期间，对话系统生成的回答如果是多个合理回答中的一个，那么对话系统将会得到奖励。在这项工作中，首先执行 SL 阶段，然后执行 RL 阶段。 在 SL 阶段，对话系统从数据集中学习不同的对话响应和行为。它有能力学习多种可能的下一个话语而不会相互矛盾/阻碍对方的学习。在 RL 阶段，对话系统可能会确定它最适合执行任务的独特行为，并在测试时也使用它。

![](https://raw.githubusercontent.com/VincentTam97/_BlogImgStorage/master/images/20190314222105.png)
